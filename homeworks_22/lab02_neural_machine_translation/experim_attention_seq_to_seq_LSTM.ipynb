{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a69610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.legacy.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'figure.figsize': (16, 12), 'font.size': 14})\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from subword_nmt.learn_bpe import learn_bpe\n",
    "from subword_nmt.apply_bpe import BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2c14144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Attention import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f1d744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edcfa14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_do_data = '../../datasets/Machine_translation_EN_RU/data_small.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eea79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get the fields for souce and target. Every sentence in in soruce and in the target \n",
    "has been tokenized, so sentence has the form: [<sos>, token_1, token_2, ..., <eos>]\n",
    "\"\"\"\n",
    "\n",
    "SRC = Field(tokenize=tokenize,\n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize=tokenize,\n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "dataset = torchtext.legacy.data.TabularDataset(\n",
    "    path=path_do_data,\n",
    "    format='tsv',\n",
    "    fields=[('trg', TRG), ('src', SRC)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08f13cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['комплекс', 'cool', 'sun', 'fully', 'furnished', 'home', 'расположен', 'в', 'городе', 'исламабад', ',', 'в', '10', 'км', 'от', 'торгового', 'комплекса', 'the', 'centaurus', '.', 'к', 'услугам', 'гостей', 'бесплатный', 'wi', '-', 'fi', '.']\n",
      "['featuring', 'free', 'wifi', ',', 'cool', 'sun', 'fully', 'furnished', 'home', 'offers', 'accommodation', 'in', 'islamabad', ',', '10', 'km', 'from', 'the', 'centaurus', 'mall', '.']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "split the dataset, every item in train, val, test has the attributes .src, .trg\n",
    "\"\"\"\n",
    "train_data, valid_data, test_data = dataset.split(split_ratio=[0.8, 0.15, 0.05])\n",
    "print(train_data[0].src)\n",
    "print(train_data[0].trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de0659bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 4000\n",
      "Number of validation examples: 250\n",
      "Number of testing examples: 750\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e4ec555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (ru) vocabulary: 1926\n",
      "Unique tokens in target (en) vocabulary: 1456\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get the vocabulary for train_data\n",
    "\"\"\"\n",
    "SRC.build_vocab(train_data, min_freq = 3)\n",
    "TRG.build_vocab(train_data, min_freq = 3)\n",
    "\n",
    "print(f\"Unique tokens in source (ru) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4693fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trg': ['the', 'tyrolean', '-', 'style', 'rooms', 'feature', 'a', 'balcony', 'with', 'mountain', 'views', ',', 'a', 'flat', '-', 'screen', 'satellite', 'tv', ',', 'and', 'a', 'bathroom', '.'], 'src': ['оформленные', 'в', 'тирольском', 'стиле', 'номера', 'располагают', 'ванной', 'комнатой', 'и', 'балконом', ',', 'с', 'которого', 'открывается', 'вид', 'на', 'горы', ',', 'а', 'также', 'в', 'них', 'предоставляется', 'телевизор', 'с', 'плоским', 'экраном', 'с', 'кабельными', 'каналами', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[19]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32b3278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8e6361bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get the iteratos sorted by length  o source sentence\n",
    "\"\"\"\n",
    "def _len_sort_key(x):\n",
    "    return len(x.src)\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device,\n",
    "    sort_key=_len_sort_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ecd20f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([25, 2]), torch.Size([19, 2]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = len(SRC.vocab)   # how many words in source vocabulary\n",
    "OUTPUT_DIM = len(TRG.vocab)  # how many words in target vocabulary\n",
    "ENC_EMB_DIM = 8\n",
    "DEC_EMB_DIM = 8\n",
    "HID_DIM = 4\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "for x in train_iterator:\n",
    "    sample_src = x.src\n",
    "    sample_trg = x.trg\n",
    "    break \n",
    "sample_src.shape, sample_trg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1bc2f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=input_dim,\n",
    "            embedding_dim=emb_dim\n",
    "        )\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hid_dim,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src [src_length, batch_size]\n",
    "        embedded = self.embedding(src)\n",
    "        embedded = self.dropout(embedded)\n",
    "        # embedded = [src_length, batch_size, emb_size]\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        # output [src_length, batch_size, hid_dim * n_directions]\n",
    "        # hidden = (hid, cell), hid =[n_layers*n_directions, batch_size, hid_dim]\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8044a092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample shape is torch.Size([25, 2])\n"
     ]
    }
   ],
   "source": [
    "print(f\"sample shape is {sample_src.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "766b7a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample shape is torch.Size([25, 2])\n",
      "enc_out shape is torch.Size([25, 2, 18])\n",
      "enc_hid shape is torch.Size([2, 2, 18])\n"
     ]
    }
   ],
   "source": [
    "enc = Encoder(input_dim = len(SRC.vocab), emb_dim = 16, hid_dim = 18, n_layers = 2, dropout = 0.5)\n",
    "enc_out, enc_hid = enc(sample_src)\n",
    "print(f\"sample shape is {sample_src.shape}\")\n",
    "print(f\"enc_out shape is {enc_out.shape}\")\n",
    "print(f\"enc_hid shape is {enc_hid[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a24e5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(num_embeddings=output_dim,\n",
    "                                      embedding_dim=emb_dim)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hid_dim,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.attention = Attention(hid_dim)\n",
    "        self.out = nn.Linear(in_features=2 * hid_dim, out_features=output_dim)        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, enc_seq):\n",
    "        # input = [batch_size]\n",
    "        input = input.unsqueeze(0)\n",
    "        # input = [1, batch_size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, batch_size, emb_dim]\n",
    "        \n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        # output = [1, batch_size, hid_dim]\n",
    "        # hidden = (hidden = [n_layers, batch_size, hid_dim], cell=[n_layers, batch_size, hid_dim])\n",
    "        \n",
    "        # input_1 for attention: query = [batch_size, trg_length=1, hid_dim]\n",
    "        # input_2 for attention: context = [batch_size, src_length, hid_dim]\n",
    "        # enc_seq is output of enc: [src_length, batch_size, hid_dim * n_directions]\n",
    "        \n",
    "        attention_output, _ = self.attention(output.transpose(0, 1),\n",
    "                                             enc_seq.transpose(0, 1))\n",
    "        \n",
    "        # attention_output = [batch_size, trg_len=1, hid_dim]\n",
    "        attention_output = attention_output.transpose(0, 1)\n",
    "        # attention_output = [trg_len=1, batch_size, hid_dim]\n",
    "        \n",
    "        prediction = self.out((torch.cat([attention_output.squeeze(0),\n",
    "                                         output.squeeze(0)], dim=1)))\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5228ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        #src = [src sent len, batch size]\n",
    "        #trg = [trg sent len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        # Again, now batch is the first dimention instead of zero\n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #we use all hidden states in the enc (enc_seq), and hidden state from last token as input to dec\n",
    "        enc_seq, hidden = self.encoder(src)\n",
    "        input = trg[0,:]\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden = self.decoder(input, hidden, enc_seq)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            input = (trg[t] if teacher_force else top1)\n",
    "        \n",
    "        return outputs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
