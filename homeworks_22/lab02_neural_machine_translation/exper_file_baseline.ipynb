{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08db8527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.legacy.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'figure.figsize': (16, 12), 'font.size': 14})\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from subword_nmt.learn_bpe import learn_bpe\n",
    "from subword_nmt.apply_bpe import BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e32fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "203c68e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', '!', 'how', 'are', 'you', 'my', 'friend', '?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "tokenize the sentence.lower(). inp: str, return: list of str\n",
    "\"\"\"\n",
    "tokenizer_W = WordPunctTokenizer()\n",
    "def tokenize(x, tokenizer=tokenizer_W):\n",
    "    return tokenizer.tokenize(x.lower())\n",
    "tokenize(\"Hi! How are you my friend?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe3ab6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_do_data = '../../datasets/Machine_translation_EN_RU/data_small.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90b72cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get the fields for souce and target. Every sentence in in soruce and in the target \n",
    "has been tokenized, so sentence has the form: [<sos>, token_1, token_2, ..., <eos>]\n",
    "\"\"\"\n",
    "\n",
    "SRC = Field(tokenize=tokenize,\n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize=tokenize,\n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "dataset = torchtext.legacy.data.TabularDataset(\n",
    "    path=path_do_data,\n",
    "    format='tsv',\n",
    "    fields=[('trg', TRG), ('src', SRC)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4e0e7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['хостел', 'находится', 'недалеко', 'от', 'пристани', 'panajachel', ',', 'где', 'работает', 'множество', 'ресторанов', 'и', 'баров', '.']\n",
      "['the', 'property', 'is', 'close', 'to', 'the', 'panajachel', 'dock', ',', 'where', 'restaurants', 'and', 'bars', 'are', 'located', '.']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "split the dataset, every item in train, val, test has the attributes .src, .trg\n",
    "\"\"\"\n",
    "train_data, valid_data, test_data = dataset.split(split_ratio=[0.8, 0.15, 0.05])\n",
    "print(train_data[0].src)\n",
    "print(train_data[0].trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85a68171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 4000\n",
      "Number of validation examples: 250\n",
      "Number of testing examples: 750\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d4962fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (ru) vocabulary: 1886\n",
      "Unique tokens in target (en) vocabulary: 1435\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get the vocabulary for train_data\n",
    "\"\"\"\n",
    "SRC.build_vocab(train_data, min_freq = 3)\n",
    "TRG.build_vocab(train_data, min_freq = 3)\n",
    "\n",
    "print(f\"Unique tokens in source (ru) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ea2ec76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trg': ['the', 'shops', 'at', 'prudential', 'center', 'is', '900', 'metres', 'from', 'encore', 'b', '&', 'b', ',', 'while', 'john', 'hancock', 'tower', 'is', '900', 'metres', 'away', '.'], 'src': ['торговый', 'центр', 'prudential', 'center', 'и', 'небоскреб', 'джона', 'хэнкока', 'находятся', 'в', '900', 'м', 'от', 'отеля', 'типа', '«', 'постель', 'и', 'завтрак', '»', 'encore', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0c02027",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78856d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get the iteratos sorted by length  o source sentence\n",
    "\"\"\"\n",
    "def _len_sort_key(x):\n",
    "    return len(x.src)\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device,\n",
    "    sort_key=_len_sort_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94c97450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 2]), torch.Size([21, 2]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = len(SRC.vocab)   # how many words in source vocabulary\n",
    "OUTPUT_DIM = len(TRG.vocab)  # how many words in target vocabulary\n",
    "ENC_EMB_DIM = 8\n",
    "DEC_EMB_DIM = 8\n",
    "HID_DIM = 4\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "for x in train_iterator:\n",
    "    sample_src = x.src\n",
    "    sample_trg = x.trg\n",
    "    break \n",
    "sample_src.shape, sample_trg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1165e2a",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aff524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "#         self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=input_dim,\n",
    "            embedding_dim=emb_dim\n",
    "        )\n",
    "            # <YOUR CODE HERE>\n",
    "        \n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hid_dim,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "            # <YOUR CODE HERE>\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)# <YOUR CODE HERE>\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src_sent_len, batch_size]\n",
    "        # Compute an embedding from the src data and apply dropout to it\n",
    "        embedded = self.embedding(src) # <YOUR CODE HERE>\n",
    "        embedded = self.dropout(embedded)\n",
    "        #embedded = [src_sent_len, batch_size, emb_dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        #outputs = [src_sent_len, batch_size, hid_dim * n_directions]\n",
    "        #hidden = [n_layers * n_directions, batch_size, hid_dim]\n",
    "        #cell = [n_layers * n_directions, batch_size, hid_dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "223a5e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_network\n",
    "Encoder = my_network.Encoder\n",
    "enc = Encoder(input_dim=len(SRC.vocab), \n",
    "              emb_dim = 8,\n",
    "              hid_dim = 4, \n",
    "              n_layers = 2,\n",
    "              dropout = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee8d4952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 2])\n",
      "enc hidden shape: torch.Size([2, 2, 4])\n",
      "enc dell shape: torch.Size([2, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "print(sample_src.shape)\n",
    "enc_hid, enc_cell = enc(sample_src)\n",
    "print(f\"enc hidden shape: {enc_hid.shape}\")\n",
    "print(f\"enc dell shape: {enc_cell.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d279b699",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f270f85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=output_dim,\n",
    "            embedding_dim=emb_dim\n",
    "        )\n",
    "            # <YOUR CODE HERE>\n",
    "        \n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hid_dim,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "            # <YOUR CODE HERE>\n",
    "        \n",
    "        self.out = nn.Linear(\n",
    "            in_features=hid_dim,\n",
    "            out_features=output_dim\n",
    "        )\n",
    "            # <YOUR CODE HERE>\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)# <YOUR CODE HERE>\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        #input = [batch_size]\n",
    "        #hidden = [n_layers * n_directions, batch_size, hid_dim]\n",
    "        #cell = [n_layers * n_directions, batch_size, hid_dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n_layers, batch_size, hid_dim]\n",
    "        #context = [n_layers, batch_size, hid_dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        #input = [1, batch size]\n",
    "        # Compute an embedding from the input data and apply dropout to it\n",
    "        embedded = self.dropout(self.embedding(input))# <YOUR CODE HERE>\n",
    "        \n",
    "        #embedded = [1, batch_size, emb_dim]\n",
    "        #output = [sent_len, batch_size, hid_dim * n_directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #sent len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.out(output.squeeze(0))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8401062",
   "metadata": {},
   "outputs": [],
   "source": [
    "Decoder = my_network.Decoder\n",
    "dec = Decoder(output_dim=len(TRG.vocab), \n",
    "              emb_dim = 8,\n",
    "              hid_dim = 4, \n",
    "              n_layers = 2,\n",
    "              dropout = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "deab956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        #trg = [trg sent len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        # Again, now batch is the first dimention instead of zero\n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            \n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            input = (trg[t] if teacher_force else top1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8bf9e272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 2]) torch.Size([21, 2])\n",
      "enc hidden shape: torch.Size([2, 2, 4])\n",
      "enc dell shape: torch.Size([2, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "print(sample_src.shape, sample_trg.shape)\n",
    "enc_hid, enc_cell = enc(sample_src)\n",
    "print(f\"enc hidden shape: {enc_hid.shape}\") # (n_layers*n_direct, batch_size, hid_size)\n",
    "print(f\"enc dell shape: {enc_cell.shape}\") # (n_layers*n_direct, batch_size, hid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83ad8c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 2 1435\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "max_len = sample_trg.shape[0]\n",
    "trg_vocab_size = OUTPUT_DIM\n",
    "print(max_len, batch_size, trg_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6baf9b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21, 2, 1435])\n"
     ]
    }
   ],
   "source": [
    "outputs = torch.zeros(max_len, batch_size, trg_vocab_size)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "381ddf26",
   "metadata": {},
   "outputs": [],
   "source": [
    " #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "dec_hid, dec_cell = enc_hid, enc_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8ce55a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "#first input to the decoder is the <sos> tokens\n",
    "input_ = sample_trg[0,:]\n",
    "print(input_.shape)\n",
    "print(input_.unsqueeze(0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5489307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(1, max_len):\n",
    "    #output = [batch size, output dim]\n",
    "        output, dec_hid, dec_cell = dec(input_, dec_hid, dec_cell)\n",
    "        outputs[t] = output\n",
    "        teacher_force = random.random() < 0.5\n",
    "        top1 = output.max(1)[1] # top1 is tensor of size [batch_size], [1] - returns indices\n",
    "        input_ = (sample_trg[t] if teacher_force else top1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1bc8de1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.6828, 0.6942], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([727, 727]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "29ce9299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([727, 727])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.max(1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "21544b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 2, 1435])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e55cfba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq_to_seq_with_Attention_LSTM import seq_to_seq_attent_LSTM\n",
    "model = seq_to_seq_attent_LSTM(len(SRC.vocab),len(TRG.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "148d0faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[-5.1898e-06,  1.0570e-05, -2.1655e-06,  ..., -1.1921e-05,\n",
       "           1.1748e-05,  1.8471e-06],\n",
       "         [ 8.4150e-06, -7.1292e-06,  3.5373e-05,  ...,  1.5775e-05,\n",
       "           1.6121e-05,  6.3456e-06]],\n",
       "\n",
       "        [[ 1.0435e-05,  2.6749e-05, -2.7406e-06,  ..., -6.3073e-06,\n",
       "           3.5059e-05, -1.0482e-05],\n",
       "         [ 7.6601e-06, -1.3873e-06,  3.7036e-05,  ..., -6.2676e-06,\n",
       "           2.1456e-05,  6.5172e-07]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-6.1668e-06, -1.0686e-05,  3.5993e-05,  ...,  7.5682e-06,\n",
       "          -3.2882e-06,  1.2210e-06],\n",
       "         [ 1.2396e-05, -2.8620e-05,  2.5354e-05,  ..., -6.9123e-07,\n",
       "          -1.1319e-05, -1.5525e-05]],\n",
       "\n",
       "        [[-1.5565e-05,  7.0127e-07,  1.9110e-05,  ...,  2.6217e-05,\n",
       "          -7.5504e-06,  1.6727e-06],\n",
       "         [ 1.7328e-05, -2.7469e-06,  1.1629e-05,  ..., -4.3854e-06,\n",
       "          -3.1500e-06, -5.7587e-06]],\n",
       "\n",
       "        [[-2.3818e-05,  1.0819e-05,  1.2289e-05,  ..., -6.6677e-06,\n",
       "          -2.5214e-05,  1.8945e-05],\n",
       "         [ 9.2975e-06,  1.3715e-05, -1.7863e-05,  ..., -5.7142e-06,\n",
       "           9.2029e-06,  2.0344e-05]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(sample_src, sample_trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e63300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bleu_score(model, test_iterator, trg_vocab): # , bert=False\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    original_text = []\n",
    "    generated_text = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, batch in tqdm.tqdm(enumerate(test_iterator)):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0)\n",
    "            output = output.argmax(dim=-1)\n",
    "\n",
    "            original_text.extend([get_text(x, trg_vocab) for x in trg.cpu().numpy().T])\n",
    "            generated_text.extend([get_text(x, trg_vocab) for x in output[1:].detach().cpu().numpy().T])\n",
    "        score = corpus_bleu([[text] for text in original_text], generated_text) * 100\n",
    "\n",
    "    return original_text, generated_text, score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
