{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ebe1d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "from torchtext.legacy.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'figure.figsize': (16, 12), 'font.size': 14})\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from subword_nmt.learn_bpe import learn_bpe\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "from train_model import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34070e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 40000\n",
      "Number of validation examples: 2500\n",
      "Number of testing examples: 7500\n",
      "Unique tokens in source (ru) vocabulary: 9260\n",
      "Unique tokens in target (en) vocabulary: 6708\n"
     ]
    }
   ],
   "source": [
    "# getting data\n",
    "path_to_data = '../../datasets/Machine_translation_EN_RU/data.txt'\n",
    "from data_preprocessing import get_dataset\n",
    "\n",
    "\n",
    "data, vocab = get_dataset(path_to_data)\n",
    "train_data, valid_data, test_data = data\n",
    "src_vocab, trg_vocab = vocab\n",
    "PAD_IDX = trg_vocab.stoi['<pad>']\n",
    "\n",
    "\n",
    "def _len_sort_key(x):\n",
    "    return len(x.src)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def get_iterators(train_data=train_data, \n",
    "                  valid_data=valid_data,\n",
    "                  test_data=test_data,\n",
    "                  batch_size=512):\n",
    "\n",
    "\n",
    "    train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data), \n",
    "        batch_size = batch_size, \n",
    "        device = device,\n",
    "        sort_key=_len_sort_key\n",
    "    )\n",
    "    return train_iterator, valid_iterator, test_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0bdaa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Attention import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba870178",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=input_dim,\n",
    "            embedding_dim=emb_dim\n",
    "        )\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, bidirectional = True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc = nn.Linear()\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        embedded = self.dropout(embedded)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        #output = [src len, batch size, enc hid dim * 2]\n",
    "        # hid =[n_layers*n_directions, batch_size, hid_dim]\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9faff89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = get_iterators(batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff446b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 2]), torch.Size([14, 2]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x in train_iterator:\n",
    "    sample_src = x.src\n",
    "    sample_trg = x.trg\n",
    "    break \n",
    "sample_src.shape, sample_trg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ef58f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample shape is torch.Size([16, 2])\n",
      "enc_out shape is torch.Size([16, 2, 36])\n",
      "enc_hid shape is torch.Size([2, 2, 18])\n"
     ]
    }
   ],
   "source": [
    "enc = Encoder(input_dim = len(src_vocab), emb_dim = 16, hid_dim = 18, dropout = 0.5)\n",
    "enc_out, enc_hid = enc(sample_src)\n",
    "print(f\"sample shape is {sample_src.shape}\")\n",
    "print(f\"enc_out shape is {enc_out.shape}\")\n",
    "print(f\"enc_hid shape is {enc_hid.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "979e0eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = nn.GRU(16, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "67e79b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedded = [1, batch size, emb dim]\n",
    "emb = torch.rand((1, 2, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3a6bb372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 18]), torch.Size([1, 2, 18]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = gru(emb, enc_hid[-1].unsqueeze(0))\n",
    "out[0].shape, out[1].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "463c6bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "attent = Attention(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "68dc8bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 18])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].transpose(0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "520b42b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 36])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_out.transpose(0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8a7e717f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-0b025bf09cc4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m attention_output = attent(out[0].transpose(0, 1),\n\u001b[0m\u001b[0;32m      2\u001b[0m                           enc_out.transpose(0, 1))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\NLP_GIT\\homeworks_22\\lab02_neural_machine_translation\\Attention.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, query, context)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# (batch_size, output_len, dimensions) * (batch_size, query_len, dimensions) ->\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;31m# (batch_size, output_len, query_len)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;31m# Compute weights across every context sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"
     ]
    }
   ],
   "source": [
    "attention_output = attent(out[0].transpose(0, 1),\n",
    "                          enc_out.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed01774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
